% Version 1.2 of SN LaTeX, November 2022
%
% See section 11 of the User Manual for version history 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst, sn-mathphys.bst. %  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys,Numbered]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>
\usepackage{booktabs}%
\usepackage{subfigure}
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%

\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{float}
%\usepackage{subcaption}
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{High-voltage Transmission Line Foreign Object and Power Component Defect Detection Based on Improved YOLOv5}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

% \author[1,2]{\fnm{Shanshan} \sur{Wang}}\email{wangshanshan@hbut.edu.cn}

% \author[1]{\fnm{Weiwei} \sur{Tan}}\email{248397032@qq.com}
% \equalcont{These authors contributed equally to this work.}

% \author[1]{\fnm{Tengfei} \sur{Yang}}
% \equalcont{These authors contributed equally to this work.}

% \author[1,2]{\fnm{Liang} \sur{Zeng}}
% \equalcont{These authors contributed equally to this work.}

% \author[3]{\fnm{Wenguang} \sur{Hou}}
% \equalcont{These authors contributed equally to this work.}

% \author*[3]{\fnm{Quan} \sur{Zhou}}\email{zhouquan910@hust.edu.cn}
% \equalcont{These authors contributed equally to this work.}

% \affil[1]{\orgdiv{School of Electrical and Electronic Engineering}, \orgname{Hubei University of Technology}, \orgaddress{\city{Wuhan}, \postcode{430068}, \state{Hubei Province}, \country{China}}}

% \affil[2]{\orgdiv{Hubei Engineering Research Center for Safety Monitoring of New Energy and Power Grid Equipment}, \orgname{Hubei University of Technology}, \orgaddress{\city{Wuhan}, \postcode{430068}, \state{Hubei Province}, \country{China}}}

% \affil*[3]{\orgdiv{College of lift Science and Technology}, \orgname{Huazhong University of Science and Technology}, \orgaddress{\city{Wuhan}, \postcode{430074}, \state{Hubei Province}, \country{China}}}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{With the outstanding performance of deep learning in the field of computer vision, the automatic visual detection of foreign bodies in transmission lines and electrical equipment defects by inspection robots and Unmanned Aerial Vehicles (UAVs) based on neural networks has become an attractive topic in smart grid. However, in practical application scenarios, small-sized target defects pose a great challenge to the detection accuracy of existing mainstream deep learning detection networks with limited perceptual fields. To solve the above problems, the paper proposes a detection model of YOLOv5 transmission line inspection image. Firstly, the key target images under different backgrounds and attitudes are collected and preprocessed. Specifically, in order to improve the perception ability of networks for small-sized targets, the K-means clustering algorithm is used to optimize the size of the anchor box, which effectively improves the fitting ability of the key target features. Then, recursive gated convolution is used as the backbone network to improve the ability to extract key target features. Finally, considering the concealment of small-scale features, the space-to-depth convolution (SPD-Conv) module is added to the neck network to realize down-sampling and retain all the information in the channel dimension. In addition, a feature prediction layer is added to optimize the scale of network detection, and a Simple Parameter-Free Attention Module (SimAM) is added to further optimize the characterization of network features. The experimental results show that the accuracy and recall of the proposed network are 96.8\% and 93.3\%, respectively. The average detection accuracy reaches 97.1\%, which is 3.8\% higher than that of YOLOv5 network, 5.2\% higher than YOLOv6 and 1.0\% higher than that of YOLOv7 network. The proposed method significantly improves the detection performance of critical targets and defects of high-voltage transmission lines. }

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Transmission line inspection, Aerial images, Object detection, Recursive gated convolution}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec1}

Insulators and dampers, as essential power components in transmission lines, play an critical part of supporting the line, blocking the current, and preventing damage to the transmission line~\cite{zhao2020detection3}. They are subject to prolonged exposure to the outside circumstances, often corroded by storms, rain, and snow, inevitably resulting in damage and causing defects. In addition, bird nest on transmission lines can lead to discharging and tripping of lines. These phenomena seriously harm the safe operation of high-voltage lines and decrease the longevity of electrical equipment~\cite{ma2021real}. Consequently, precisely identifying and positioning the defects and foreign bodies on the electrical equipment on the transmission line has a profound significance for ensuring the healthy operation of the power system~\cite{mohamed2020unmanned}.

Traditional transmission line inspection often relies on manual inspection, but high-voltage transmission lines cover a wide range and have complex terrain. It requires staff to observe closely, not only does it consume a considerable amount of manpower and material resources, but the efficiency and safety are also very low. Recently, with the progress of UAV technology and the increasing range of applications, the transmission line images collected by UAVs  is becoming  increasingly popular~\cite{liang2020detection, zhang2004mobile, zhang2017uav,li2023focus}.

At present, vision-based methods for detecting the health status of transmission lines are gradually emerging. It mainly uses image processing methods to identify and locate foreign objects and electrical equipment and defects on transmission lines to achieve daily inspection of transmission lines. In addition, compared with other types of detection objects, transmission line defect detection has the characteristics of large target differences and complex backgrounds.

The traditional target detection algorithm is usually accomplished by two relatively independent processes of feature extraction and feature recognition. Image feature extraction is mainly based on manual feature extraction, which depends on the size, shape, texture and other features of the target. Feature recognition methods mainly include histogram of oriented gradient (HOG)~\cite{zaidi2022survey}, scale-invariant feature transform (SIFT)~\cite{tulbure2022review} and so on. For instance,  two common hough transform techniques and a line tracing algorithm are used in \cite{wei2022composite} to realize the detection of power line. In \cite{tan2020catenary}, a methodology for classifying insulator damage conditions is proposed through the matching and fusion of contour features and gray similarity. However, traditional methods have low computational complexity and slow processing speed, and are easily affected by complex background and illumination conditions.

Compared with traditional methods, the deep learning object detection methods represented by convolutional neural network can automatically learn image features that are difficult for human observation from a great deal of image, and are applicable to multiple types of scenes at the same time. In target detection, there are two main types of neural networks that are commonly used.  The first type is a two-stage model that relies on region extraction, such as region-convolutional neural networks (Faster R-CNN)~\cite{ren2015faster}. The second type is a single-stage model that uses regression, such as you only look once (YOLO) series~\cite{redmon2016you}, single shot detector (SSD)~\cite{liu2016ssd}, etc. These two types of models have been widely used in target detection. Using feature pyramids, an enhanced Faster R-CNN model is proposed in \cite{zhao2021insulator}, which uses straight line detection, projection and other methods to complete insulator fault detection with complex background. In \cite{qiu2022detection}, an improved lightweight YOLOv4 model is proposed to realize insulator detection combined with transfer learning method, achieving 97.26\% accuracy. By optimizing the detection network of YOLOv4 model, the extraction of small target features in transmission line inspection images is enhanced in \cite{liu2022key}. An enhanced YOLOv4 insulator defect detection model is proposed in \cite{hao2022insulator}, which designed a new backbone network based on residual split-attention networks (ResNest)~\cite{zhang2022resnest}, and adopted multi-scale bidirectional feature pyramid to overcome the obstacle of small-size insulator defects detection. In \cite{li2022insulator}, an image enhancement method based on illumination correction and compensation is proposed to enable real-time monitoring of insulator defects detection using YOLOv5. 

The bulk of of these methods mentioned above are designed based on the detection task of insulators defective. They have highly personalized characteristics, and it is tough to achieve the expected effect on multiple-category defect detection of small targets. In this case, this paper proposes an improved YOLOv5 transmission line safety detection model for foreign object and power component defect. The primary contributions are as follows:


(1) We collect and  process a dataset named NID dataset suitable for transmission line inspection, which includes bird nests, defective insulators and dampers.

(2) By introducing recursive gated convolution, the extraction ability of target defects in detected images is improved, and the traditional sampling process is replaced by slicing operations to reduce information loss. In addition, a new prediction layer is added to the target detection network to enrich context information and further optimize the detection accuracy of the network. 

(3) Experiments show that the model can effectively improve the test accuracy of the key goals and defects in the transmission line in complex contexts to achieve a better identification effect. Apart from this, the generalizability of this method is validated on the CPLID.



The remainder of this paper is arranged as follows. Section~\ref{sec2} describes the relevant work of object detection, including the principle of recursive gated convolution and the elaborate design of the object detection framework. The data set and experimental results are introduced detailly in Section~\ref{sec3}, and Section~\ref{sec4} gives the conclusion.

\section{Methods}
\label{sec2}  % \label{} allows reference to this section

\subsection{Recursive Gated Convolution}

Recursive gated convolution, proposed in \cite{rao2022hornet}, is mainly composed of standard convolution, linear projection, and elemental multiplication. High-order spatial interaction is performed by gated convolution and recursive design, with input adaptive spatial mixing similar to the self-attention mechanism. The core mainly includes gated convolution and cyclic recursive structure design, specifically as follows:

(1) Gated Convolution
\begin{figure}[h]%
\centering
\includegraphics[width=0.4\textwidth]{fig1.png}
\caption{The structure diagram of gated convolution}\label{fig1}
\end{figure}

Figure~\ref{fig1} shows the structure of gated convolution ($gConv$), which mainly consists of projection layers (Proj), depth-wise convolution layers (DWConv), and multiplication of elements (Mul). \emph{C} represents the number of channels. * represents other feature information such as the height and width of the feature map.

For example, given an input feature of $x \in {R^{HW \times C}}$, \emph{H} and \emph{W} represent the height and width of the feature map. The output $y = gConv\left( x \right)$ of the gated convolution can be expressed as:
\begin{equation}
[p_0^{HW \times C},q_0^{HW \times C}] = {\phi _{in}}(x) \in {R^{HW \times 2C}},
\end{equation}
\begin{align}
{p_1} = f\left( {{q_0}} \right) \odot {p_0} \in {R^{HW \times C}},y = {\phi _{out}}({p_1}) \in {R^{HW \times C}}
\end{align}

where, ${\phi _{in}}$ and ${\phi _{out}}$ are the projection layer for performing channel mixing, $f$ represents the depth-wise convolution layer, and $ \odot $ refers to the multiplication of elements. The interaction between adjacent features ${p_0}$ and ${{q_0}}$ is introduced through element multiplication, so $gConv$ can be regarded as a first-order interaction.

(2) Recursive Gated Convolution

The recursive gated convolution (${g^n}Conv$) is designed based on the gated convolution to realize the higher-order interaction of long-range modeling and recursive gating. The details are shown in Fig.~\ref{fig2}.

\begin{figure}[h]%
\centering
\includegraphics[width=0.5\textwidth]{fig2.png}
\caption{The structure diagram of recursive gated convolution}\label{fig2}
\end{figure}

In Fig.~\ref{fig2}, given the input $x$ , a set of projection features ${p_0}$ and $\left\{ {{q_k}} \right\}_{k = 0}^{n - 1}$ are obtained through the projection layer, the formula can be described as:
\begin{equation}
[p_0^{HW \times {C_0}},q_0^{HW \times {C_0}},...,q_{n - 1}^{HW \times {C_{n - 1}}}] = {\phi _{in}}(x) \in {R^{HW \times ({C_0} + \sum\nolimits_{0 \le k \le n - 1} {{C_k}} )}}
\end{equation}

The gated convolution is recursively executed to gain the output of multiplication of elements at all levels as follows:

\begin{align}
{p_1} = f({q_0}) \odot {g_0}({p_0})/a,    k = 0 
\end{align} 
\begin{equation}
{p_2} = f({q_1}) \odot {g_1}({p_1})/a,    k = 1 
\end{equation}
\begin{equation}
\cdots  \cdots \nonumber
\end{equation}
\begin{equation}
{p_n} = {f_{n - 1}}({q_{n - 1}}) \odot {g_{n - 1}}({p_{n - 1}})/\alpha ,   k = n - 1 
\end{equation}

In the above formula, $f$ is the 7×7 Depth-wise convolution,and then, the output is proportionally resized to $1/\alpha $ to stabilize the training, and $\left\{ {{g_k}} \right\}$ is used to match the dimensions in different orders, which is specifically expressed as :

\begin{equation}
{g_k} = \left\{ {\begin{array}{*{20}{c}}
{Identity,}&{k = 0,}\\
{Linear({C_{k - 1,}}{C_k})}&{0 \le k \le n - 1}
\end{array}} \right.    
\end{equation}

The output result ${p_n}$ of the final recursive iteration is fed into the Proj mapping layer to obtain the outcome of ${g^n}Conv$ .

\subsection{Improvement of the Feature Extraction Part}

Compared with the two-stage detection network, YOLO network has the benefits of light network structure and rapid detection speed. YOLOv5 is one of the mature YOLO models at present, so the paper chooses YOLOv5 as the backbone of the detection network.

Cross-stage-partial-connections (CSP)Darknet~\cite{wang2020cspnet}  network is used in YOLOv5 to extract features, which achieves the extraction of target feature information through the connection of residual blocks and the stacking of convolutional blocks. However, while accelerating reasoning, the CSP structure ignores the spatial correlation of feature information, which is poor in capturing features of small targets, and there is still scope for improving feature extraction tasks.

In this paper, the feature extraction network is built by alternating connection between convolution batch normalization Sigmoid (CBS) module and feature extraction module named C3CG and spatial pyramid pooling fast (SPPF) structure. CBS module consists of a convolutional layer, a normalization layer, and the activation function. In the detail, C3CG module is designed into two branches, and its structure is shown in Fig.~\ref{fig3}. In the figure, branch 1 contains only one convolutional module; branch 2 adopts the four-stage architecture design of transformer, consisting of convolutional modules, layer normalization (Layer Norm), multi-layer perceptron (MLP), and attention modules. Recursive gated convolution (${g^n}Conv$) has higher-order spatial interaction and long-range, while ordinary convolution ignores the correlation on the feature space, so the attention module is replaced by ${g^n}Conv$ module to achieve higher-order feature interaction and complete the extraction from shallow features to deep features. In order to further optimize the higher-order feature information, convolutional block attention module (CBAM)~\cite{woo2018cbam} is introduced to merge residual information and filter out useless channel and spatial information. Finally, the two branches are spliced by the concatenate operation, and the higher-order and lower-order feature information is integrated by convolution again to ensure the integrity of the information.

\begin{figure}[h]%
\centering
\includegraphics[width=0.9\textwidth]{fig3.png}
\caption{The structure diagram of C3CG}\label{fig3}
\end{figure}

\subsection{Design of Feature Fusion Modul}

The feature extraction network in the YOLOv5 contains 53 layers in total. When the input image size of the target detection network is 640×640×3 pixels, the network will sample the original input image by eight times down-sampling, sixteen times down-sampling, thirty-two times down-sampling to get three different sizes of feature graphs, which are (80, 80, 256), (40, 40, 512), (20, 20, 1024). However, these three feature layers are present in the backbone part, situated at the middle, lower middle, and bottom layers, using only this three-layer feature layer as a detection head has low resolution and lacks the underlying feature representation.

\begin{figure}[h]%
\centering
\includegraphics[width=0.8\textwidth]{fig4.png}
\caption{The structure diagram of FPN and PAN}\label{fig4}
\end{figure}

To further improve the detection performance of small goals and subtle defects, a 160×160 feature scale module is added to the feature fusion module. By up-sampling the 80×80 feature map and then fusing with the four times down-sampling feature map of the raw image, the output feature scale is expanded from 80×80 to 160×160. Finally, the obtained multi-scale feature maps are aggregated in the output part for prediction. In addition, the SimAM~\cite{yang2021simam} is introduced into the feature fusion structure attention mechanism to calculate the 3D weights among the features without adding additional parameters, while speeding up the feature fusion. The great details of the improved feature fusion module, which is composed of a top-down feature pyramid network (FPN)~\cite{lin2017feature} and a bottom-up path aggregation network (PAN)~\cite{li2018pyramid}, are shown in Fig.~\ref{fig4}.

The neck network feature fusion module introduces the SPD-Conv~\cite{sunkara2022no} layer to replace the ordinary convolutional layer with a step size of 2 to avoid losing too much small target information during the down sampling process. SPD-Conv can reduce redundant pixel information in the difficult task of blurred images or small objects by achieving down-sampling of the feature map through a slice-like operation, as shown in Fig.~\ref{fig5}.

\begin{figure}[h]%
\centering
\includegraphics[width=0.8\textwidth]{fig5.png}
\caption{The structure diagram of SPD-Conv}\label{fig5}
\end{figure}

Where \emph{S} represents the height and width of the feature graph, and \emph{C} represents the channel. An input feature map of size $S \times S \times {C_1}$ is sliced into 4 sub-feature maps according to an interval sampling unit, and these sub-feature maps are connected along the channel to obtain the intermediate feature map of size $\left( {S/2 \times S/2 \times 4{C_1}} \right)$. Through a non-stride convolution, the feature map is converted from $\left( {S/2 \times S/2 \times 4{C_1}} \right)$ to $\left( {S/2 \times S/2 \times {C_2}} \right)$. The network retains more feature information and enhances the network feature expression capability.

\subsection{Improved Target Detection Model}

With the improvements presented in the previous two sections, an improved YOLOv5 model is proposed based on the architecture of YOLOv5 target detection network, which improves backbone network, neck network and detection head respectively, the overall structure improvement is revealed in Fig.~\ref{fig6}. The 3-channel transmission line inspection images acquired by the UAVs are used as input, and the source images are scaled to 640×640 pixels by adaptive scaling at the network input end. In the improved YOLOv5, C3CG module designed by recursive gated convolution method and the convolution module (CBS) are alternately connected to construct the backbone extraction network, which realizes the extraction of more effective feature information through the interaction of higher-order spatial features. After five times of feature down-sampling, the 20×20 feature map is gathered, and the feature map is passed through SPPF to further increase the receptive field of the image. Then, a top-down feature pyramid structure and a bottom-up path aggregation network are used to transfer feature information. In the neck network design, SimAM attention mechanism and 160×160 feature scale are added to enhance shallow information extraction. In particular, sliced convolution SPD-Conv is used to realize down-sampling, avoid the loss of effective feature information, and enhance the detection accuracy of small targets. 

\begin{figure}[h]%
\centering
\includegraphics[width=0.9\textwidth]{fig6.png}
\caption{The structure of foreign object and power component defect detection network for high-voltage transmission lines with improved YOLOv5}\label{fig6}
\end{figure}

\section{Experiment Analysis}\label{sec3}

The hardware configuration of all experimental environments in this paper include Intel(R) Core(TM) i5-12600KF, NVIDIA GeForce RTX 3060 Ti GPU, and the software environment is Windows11 system, Python3.8, PyTorch1.13.0 and CUDA11.3.

In the experimental setup of this study, the initial learning rate is set to 0.01 and is dynamically adjusted using the stochastic gradient descent (SGD) optimizer with momentum size of 0.937 and optimizer weight decay of 0.0005. All experiments are repeated 300 times, with a batch size of 4 and without using pre-trained weights. Additionally, the proposed method in this study uses the same image preprocessing techniques as other comparative algorithms, including Mosaic, random scaling, and horizontal flipping. Moreover, K-means algorithm is used to adjust the anchors of other algorithms.

\subsection{Evaluation Metrics}

Four metrics, precision (P), recall (R), average precision (AP), and mean average precision (mAP), are used to verify the validity and accuracy of the transmission line multi-target detection system. Higher evaluation metrics indicate better performance, and the formulas for the above measures are shown below, respectively. 

\begin{equation}
P = \frac{{TP}}{{TP + FP}}
\end{equation}

\begin{equation}
R = \frac{{TP}}{{TP + FN}}
\end{equation}

\begin{equation}
AP = \int_0^1 {P\left( R \right)} dR
\end{equation}

\begin{equation}
 { {mAP =\sum\limits_ {n = 1}^N {AP\left(n\right)}}\mathord {\left/
 {\vphantom { {mAP =\sum\limits_ {n = 1}^N {AP\left(n\right)}}N}}\right.
\kern-\nulldelimiterspace}N}
\end{equation}

where TP is the number of positive samples identified as positive samples; FP is the number of negative samples misclassified as positive samples; FN is the number of positive samples misclassified as negative samples. AP refers to the area of the P-R curve, The mAP is the average accuracy of all categories; N is the number of detection categories. 

\subsection {Experimental Dataset}

In the current assignment of inspecting transmission line images, there is a lack of publicly available data sets, and some publicly available data sets are of the same quality in terms of detection categories, with relatively single detection objects. For this purpose, combined with the China power line insulator data set (CPLID)\cite{tao2018detection}, a new data set named NID data set is collected and sorted out, some images come from the power supply bureau, which contains 400 images in three categories: bird nest, insulator, and damper. For the purpose of improving identification accuracy, the number of samples is expanded to 1600 images by rotation, translation, and random clipping, which including 1749 bird nest samples, 4961 insulator samples, and 3912 damper samples. A single image contains multiple detection objects. The part of the data sample enhancement is shown in Fig.~\ref{fig7}. The NID data set is partitioned randomly into into training set, validation set and testing set according to 7:2:1, and the numbers are 1120, 320 and 160, respectively. And the label categories are divided into bird nest (nest), insulator, cracked insulator (break), insulator with defect (defect), damper and damper with defect (defective damper), the details are shown in Table ~\ref{tab1}.

\begin{figure}[htbp]
	\centering
	\subfigure[ Original image]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[scale=0.9]{fig7a.png}
		\end{minipage}%
	}%
	\subfigure[ Rotate 180 degrees]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[scale=0.9]{fig7b.png}
		\end{minipage}%
	}%
	\\
	\subfigure[ Translation]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[scale=0.9]{fig7c.png}
		\end{minipage}%
	}%
	\subfigure[ Random clipping]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[scale=0.9]{fig7d.png}
		\end{minipage}%
	}%
	
	\centering
	\caption{Example of images with data augmentation}\label{fig7}
\end{figure}

\begin{table}[h]
\caption{Data set partition table}\label{tab1}%
\begin{tabular}{@{}llll@{}}
\toprule
Label category 1 & Training set  & Validation set & Testing set\\
\midrule
                nest                 & 1223	& 349   & 177  \\
                insulator            & 3472	& 992	& 497  \\
                break                & 771	& 220	& 111  \\
                defect               & 336	& 96	& 48   \\
                damper               & 2738	& 782	& 392  \\
                defective damper     & 952	& 275	& 134  \\
\botrule
\end{tabular}
\end{table}


\subsection {Data Analysis}\label {subsec2}

In the data set, there are insulator targets with large aspect ratio, defect targets with small size and dampers, which generally show the characteristics of large difference in targets. The a priori anchor frame parameters based on the common objects in context (COCO) data set would no longer be applicable to this data set, so K-means clustering is used to optimize the a priori anchor frame parameters. The clustering analysis is performed on the a priori frame of the detection objects in the data set, and the clustering results are shown in Fig.~\ref{fig8}. Different colors represent different clusters, × represents the cluster core.

\begin{figure}[h]%
\centering
\includegraphics[width=0.8\textwidth]{fig8.png}
\caption{The graph of K-means clustering results}\label{fig8}
\end{figure}

The specific steps of K-means clustering are as follows:

1) First, determine the number of clusters \emph{k}, establish a coordinate system with the width of the anchor frame as the horizontal axis and the height as the vertical axis, and then divide the data set into \emph{k} clusters;

2) The ratio of the width and height of the anchor frame in each group is randomly selected as the initial centroid of the population;

3) Calculate the distance from the coordinates of all samples to each center of mass, divide them into the nearest clusters, and update the center of mass of each cluster as the expected value of the samples in the cluster;

4) Repeat step 3) until the center of mass of each cluster no longer changes.

In this paper, 4 prediction scales are used, in the feature diagram, three anchor frames are placed for each target to predict, so \emph{k} is selected as 12 and the anchor frame parameters corresponding to different scales are obtained as shown in Table ~\ref{tab2}:

\begin{table}[h]
\caption{Size distribution of anchor frame after improvement}\label{tab2}%
\begin{tabular}{@{}cl@{}}
\toprule
Cell size /(pixels × pixels)  & Anchor size/(pixels × pixels)\\
\midrule
                    160	& (92×80),(53×218),(155×91) \\
                    80	& (21×86),(53×56),(26×185)  \\
                    40	& (23×28),(31×23),(33×38)   \\
                    20	& (15×13),(12×29),(22×18)   \\
\botrule
\end{tabular}
\end{table}

\subsection {Results}\label{sec4}

\subsubsection {Effect of Improved Methods}\label {subsubsec2}

To verify the effectiveness of the improvement measures proposed in the paper, eight models are set up for ablation experiments. The experimental results are shown in  Table~\ref{tab3}.

\begin{table}[h]
\caption{Size distribution of anchor frame after improvement}\label{tab3}%
\begin{tabular}{@{}lccccccc@{}}
\toprule
 & C3CG	& PAnet-P	& SPD-Conv	& SimAM	& P(\%)& R(\%)	& mAP(\%)\\
\midrule
Model 1	 &	&  &  &                            & 92.2	 & 85.4	    & 93.3\\
Model 2	 & \checkmark &	 &   &                 & 95.5   & 89.7	   & 94.6\\
Model 3  & 	& \checkmark  &    &               & 93.9   & 88.6	   & 94\\
Model 4  & 	&  & \checkmark	  &	               & 94.6   & 92.1	   & 95.1\\
Model 5  &	&  &   & \checkmark	               & 93.7   & 91.8	   & 94.2\\
Model 6  &	&  & \checkmark & \checkmark	   & 96.2   & 90.9     & 96.1\\
Model 7  &	& \checkmark & \checkmark & \checkmark   & 97.7   & 92.6     & 96.7\\
Model 8  &\checkmark & \checkmark & \checkmark & \checkmark & 96.8   & \textbf{93.3}     & \textbf{97.1}\\
\botrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
	\centering
	\subfigure[ Loss curve]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[scale=0.38]{fig9a.eps}
		\end{minipage}%
	}%
	\subfigure[ mAP curve]{
		\begin{minipage}[t]{0.5\linewidth}
			\centering
			\includegraphics[scale=0.38]{fig9b.eps}
		\end{minipage}%
	}%
	\centering
	\caption{The curves of training losses and mAPs}\label{fig9}
\end{figure}


In Table~\ref{tab3}, The leftmost column represents the network model number, the last three columns represent the evaluation metrics, and the hook symbol in the table represents that the network has incorporated the corresponding improvement strategy. In the table, Model 1 does not incorporate any improvement strategy, and the mAP value is 93.3\%. Model 2 incorporates the optimization strategy of constructing a feature extraction network using C3CG modules, resulting in a 1.3\% increase in mAP. It has improved the performance of the network to some extent, that is because the recursive gated convolution pays more attention to the spatial correlation of feature information and can better capture defect information in the detected objects, such as cracked insulators. Model 3 represents the addition of a feature prediction layer, which improves the detection accuracy by 0.7\% compared to Model 1. The lower-level features contain more edge information, and adding a new prediction scale can enrich the context information, which is beneficial for object detection to perform position regression. Model 4 introduces SPD-Conv with a scaling factor of 2 for down-sampling. SPD-Conv can retain all the information in the channel dimension during down-sampling, enabling the network to fully utilize the feature information extracted from the backbone network. Both Model 2 and Model 4 are aimed to increase the utilization rate of feature information and improve the detection capability for small objects, thus enhancing the overall performance of the network. Besides, Model 5 builds upon Model 1 by introducing an SimAM mechanism, which allows the network to focus on important features and suppress irrelevant ones during the object detection process, resulting in a 0.9\% increase in mAP. It is worth noting that Model 6 adds the SimAM mechanism after SPD-Conv on the basis of Model 4, and mAP increases from 95.1\% to 96.1\%, that is because SimAM can accelerate feature fusion after SPD-Conv without increasing network parameters. Based on this, Model 7 adds the feature prediction layer, increasing mAP to 96.7\%, and reaching the optimal P-value of 97.7\%. Our model, i.e. Model 8, achieves an R of 93.3\%, an mAP of 97.1\%, and obtains a suboptimal P-value. Considering all the quantitative metrics, the proposed method demonstrates the best performance. In addition, to better observe the entire training process of the ablation experiments, we plot the curves of loss and mAP as shown in Fig.~\ref{fig9}.


\subsubsection {Comparison with Other Defect Detection Methods}

In order to quantitatively evaluate the detection advantages of the improved YOLOv5 network proposed, the detection performance of the proposed algorithm is compared with that of Faster R-CNN, SSD, YOLOv5, YOLOv6 and YOLOv7, which are popular in engineering applications. The comparison results of each network are shown in Table ~\ref{tab4} and Fig.~\ref{fig10}.

\begin{table*}[htbp]%%%%表5
				%\scriptsize%%调整字体
				%\centering
				%\captionsetup{singlelinecheck=off}
				%\captionsetup{singlelinecheck=off}
				\caption{Comparative experiments}
				\resizebox{\textwidth}{!}{
					\begin{tabular}{lccccccccc}
					\toprule
					& \multicolumn{6}{@{}c@{}}{AP/\%} \\\cmidrule{2-7}%
Methods & nest	& insulator & break & defect & damper & defective damper & P/\% & R/\% &mAP@0.5/\%\\
\midrule
Faster R-CNN	& 95.6	& 87.1	& 47.9	& 60.2  & 73.8	& 84.3	& 85.9 & 73.5 & 74.8\\
SSD	            & 92.3	& 84.7	& 46.1	& 58.0	& 72.3	& 79.9	& 70.6 & 61.7 & 72.1\\
YOLOv5     	    & 99.4	& 97.2	& 70.3	& 99.5	& 94.9	& 95.8  & 92.4 & 87.9 &	93.3\\
YOLOv6	        & 99.1	& 96.9	& 78.3	& 94.1	& 95.2	& 88.4	& 93.2 & 88.8 & 91.9\\
YOLOv7	        & 99.5	& 98.5	& 87.7	& 99.0	& 95.7	& 96.3	& 95.5 & 93.7 & 96.1\\
Proposed method	& 99.5	& \textbf{98.8}	& \textbf{92.6}	& 98.2	& \textbf{96.6}	& \textbf{97.1}	& \textbf{96.8}  & 93.3 & \textbf{97.1} \\
					\bottomrule
					
					
				\end{tabular}%
			}
		\label{tab4}%
	\end{table*}%

In Table ~\ref{tab4}, the average accuracy of the Faster R-CNN is 74.8\%, which is higher than that of the SSD. Generally speaking, two-stage object detection networks typically achieve higher detection accuracy compared to one-stage networks. This is due to the two-stage approach using a two-part architecture to handle the problem of class imbalance and RPN (Region Proposal Network) to balance positive and negative samples when fitting bounding boxes. By using a two-stage cascaded approach, these networks are able to more accurately detect objects in complex and cluttered scenes. However, with the iterative optimization of YOLO series detection network, the accuracy of network detection is constantly improved. Among them, the mAP value of YOLOv5 on the NID dataset reaches 93.3\%, which represents a significant improvement over Faster R-CNN and SSD. This performance boost is attributed to the integration of a series of optimization techniques, including the application of the focal loss function, which significantly enhances the capability of the network in object detection. Notably, YOLOv5 achieves the highest AP value of 99.5\% for insulators with defects among all the algorithms tested. The P and R of YOLOv6 are 93.2\% and 88.8\% respectively. The average detection accuracy is 91.9\%, which is worse than YOLOv5. The mAP scores for YOLOv7 and the proposed method are 96.1\% and 97.1\%, respectively. In particular, both achieve a detection accuracy of 99.5\% for the bird nest category. Moreover, the proposed method demonstrates high detection accuracy across multiple categories, achieving an AP value of 92.6\% for small damaged insulators, which represents a significant improvement of 19.6\% over that of YOLOv5.

\begin{figure}[h]%
\centering
\includegraphics[width=0.85\textwidth]{fig10.png}
\caption{The variation curves of mAPs}\label{fig10}
\end{figure}

Fig.~\ref{fig10} presents the variation curves of the mAPs of the network models during the training process. From the figure, Faster R-CNN converges the slowest and becomes stable at around 200 epochs. The mAP curve of SSD is below other curves and stays around 72\% at the final value. YOLOv5, YOLOv6, YOLOv7, and the improved YOLOv5 converge gradually at about 100 epochs, and the convergence of the improved network is relatively fast, and the average detection accuracy curve finally stays at about 97\%. It can be concluded that the improved network has higher parameters and better training advantages.

\subsubsection {Qualitative Analysis}

\begin{figure}[htbp]
	\centering
	\subfigure[ Faster R-CNN]{
		\begin{minipage}[t]{0.325\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig11a.png}
		\end{minipage}%
	}%
	\subfigure[ SSD]{
		\begin{minipage}[t]{0.325\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig11b.png}
		\end{minipage}%
	}%
	\subfigure[YOLOv5]{
		\begin{minipage}[t]{0.325\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig11c.png}
		\end{minipage}%
	}%
 \\
	\subfigure[YOLOv6]{
		\begin{minipage}[t]{0.325\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig11d.png}
		\end{minipage}%
	}%
        \subfigure[ YOLOv7]{
		\begin{minipage}[t]{0.325\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig11e.png}
		\end{minipage}%
	}%
	\subfigure[Ours]{
		\begin{minipage}[t]{0.325\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig11f.png}
		\end{minipage}%
	}%
	
	\centering
	\caption{The detection results under small color difference conditions}\label{fig11}
\end{figure}

In this section, a selection of typical outdoor transmission line images is screened from the testing set to test the proposed method, and compared with Faster R-CNN, SSD, YOLOv5, YOLOv6, and YOLOv7 algorithms. It is worth noting that to better observe the detection results, we enlarges the detected images to different scales. 

Figure~\ref{fig11} shows the detection results of the general scene in transmission line inspection. As seen from Fig.~\ref{fig11}f, the proposed method recognizes all objects in the image. And except for our algorithm, the defective damper in the lower right position of the image is not detected by other algorithms, because the damper itself has a similar color to the soil and the target is too small.   Particularly, in Fig.~\ref{fig11}a and b, due to the small color difference between the target and the background, Faster R-CNN and SSD misjudges the intact damper as one with defects.

\begin{figure}[htbp]
	\centering
	\subfigure[ Faster R-CNN]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig12a.png}
		\end{minipage}%
	}%
	\subfigure[ SSD]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig12b.png}
		\end{minipage}%
	}%
	\subfigure[YOLOv5]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig12c.png}
		\end{minipage}%
	}%
 \\
	\subfigure[YOLOv6]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig12d.png}
		\end{minipage}%
	}%
        \subfigure[ YOLOv7]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig12e.png}
		\end{minipage}%
	}%
	\subfigure[ Ours]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig12f.png}
		\end{minipage}%
	}%
	
	\centering
	\caption{The detection results under complex and occlusive conditions}\label{fig12}
\end{figure}

Figure \ref{fig12} illustrates the detection results of Faster R-CNN, SSD, YOLOv5, YOLOv6, YOLOv7, and the proposed method under a complex detection environment with occlusions. The results indicate that due to occlusion by the high-voltage tower, the insulators on the right side of the image center are not accurately recognized by Faster R-CNN, SSD, and YOLOv6 algorithms. In fact, YOLOv6 incorrectly identifies them as damaged insulators. Furthermore, the small color difference between the two dampers near the right side of the image and the high-voltage tower, as well as the small size of the defective insulator(defect) at the bottom of the image, make it impossible for Faster R-CNN and SSD to recognize them. YOLOv5 also misses one damper in this context. Especially for the damaged insulator(break) on the left side of the image center, all algorithms, including Faster R-CNN, SSD, YOLOv5, YOLOv6, and YOLOv7, fail to accurately locate the detection object owing to its small size. In contrast, as shown in Fig.\ref{fig12}f, the proposed algorithm successfully detects all objects with a high localization accuracy.

What is more, we conduct additional tests to compare the efficacy of the proposed algorithm under low-lighting conditions, the detection results are shown in Fig.~\ref{fig13}. Our method can locate the faulty insulator accurately, demonstrating superior detection performance than other algorithms. While YOLOv6, Faster R-CNN, and SSD only recognize limited objects, such as bird nest and insulator, failing to identify the defective damper. Nevertheless, the proposed method also misses several detection objects in Fig.~\ref{fig13}f, which are represented by dashed ellipses. This is because the lighting conditions of the test image are poor, resulting in very low contrast between the damper and the background color. Instances of such situations are relatively infrequent during training, resulting in the inability of network to detect them. In addition, more comparative test results are shown in the \hyperref[Appendix]{Appendix}.

\begin{figure}[htbp]
	\centering
	\subfigure[ Faster R-CNN]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.245]{fig13a.png}
		\end{minipage}%
	}%
	\subfigure[ SSD]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.245]{fig13b.png}
		\end{minipage}%
	}%
	\subfigure[YOLOv5]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.245]{fig13c.png}
		\end{minipage}%
	}%
 \\
	\subfigure[YOLOv6]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.245]{fig13d.png}
		\end{minipage}%
	}%
        \subfigure[ YOLOv7]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.245]{fig13e.png}
		\end{minipage}%
	}%
	\subfigure[Ours]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.245]{fig13f.png}
		\end{minipage}%
	}%	
 
	\centering
	\caption{The detection results under low-lighting conditions}\label{fig13}
\end{figure}


\subsubsection {Comparison with Related Works}
In this section, to further validate the effectiveness of our algorithm in this paper, experiments will be conducted on the CPLID~\cite{tao2018detection}. The China power line insulator data set (CPLID) is provided by the China Electric Power Research Institute and contains 600 normal insulator images and 248 synthetic defective insulator images. It is a dataset used for insulator defect detection.

\begin{table}[h]
\caption{Comparison with related works on the CPLID}\label{tab5}
\begin{tabular*}{\textwidth}{@{\extracolsep\fill}llllll}
\toprule%
NO. & Work & Method & P/\% & R/\% & mAP/\%  \\
\midrule
1 & Tao et al.~\cite{tao2018detection} & ILN & 90.4 & 96.6 & 93.4 \\
& & DDN & 91.0 & 95.8 & 93.3\\
2 & Li et al.~\cite{li2020insulator} & Faster R-CNN + OHEM & Not given & Not given & 90.75\\
3 & Ding et al.~\cite{ding2022high} & YOLOv5 (EIoU+AFK-MC2)  & Not given & Not given & 96.0\\
4 & Wang et al.~\cite{wang2023insulator} & YOLOv4 + data augmentation & 91.0 & 98.84 & 99.08\\
5 & The proposed method & Improved YOLOv5 & 98.6 & 94.3 & 97.8\\
\botrule
\end{tabular*}
\end{table}

Table \ref{tab5} presents the comparison results between our proposed method and other related works. In this table, four reference results on the CPLID are selected and named NO.1-NO.4, and NO.5 represents the experimental results of our proposed method. The last three columns represent the evaluation metrics of P, R, and mAP values, where ``Not given" indicates that the corresponding values are not provided in the original work. The table reveals that Wang et al.~\cite{wang2023insulator} achieves the highest mAP value with a score of 99.08\%, surpassing the performance of our algorithm by 1.28\%. Among them, our method has the highest P value on the CPLID, which is 98.6\%.


\section {Conclusion}\label {sec4}

The paper proposes a deep learning detection network based on recursive gating design, which is used for intelligent defect detection of foreign object and power component defect in high voltage transmission lines. Firstly, the key target and defect data sets of high voltage transmission lines are constructed by collecting and collating, which is more suitable for the complex on-site detection environment. Secondly, the K-means algorithm is used to optimize the anchor box of the data set to improve the fitting ability of the network to the detection target. Then, a C3CG module is designed to construct a backbone network based on the correlation of high-order spatial features of recursive gated convolution. Finally, the feature detection scale is added on the original basis, the underlying feature information is fused, and the down-sampling is realized by slice convolution SPD-Conv, and the attention mechanism is added to optimize the detection network. Experimental results show that the detection precision and recall rate of key targets and defects of transmission lines under complex environments are 96.8\% and 93.3\%, respectively, with mAP of 97.1\%. It has high precision and robustness in the outdoor complex detection environment, improves the inspection image detection efficiency and accuracy of transmission lines, and provides a new idea for the application and development of smart grid.

\section*{Declarations}

\begin{itemize}
\item \textbf{Conflict of interest}: The authors declare no competing interests.
\end{itemize}


\section*{Appendix}\label{Appendix}

\begin{appendices}
%\section{Appendix}

The comparative test results between the proposed method and other algorithms.



\setcounter{figure}{13}
%\renewcommand{\thefigure}{\thechapter.\arabic{figure}}
\begin{figure}[H]
	\centering
	\subfigure[ Faster R-CNN]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig14a.png}
		\end{minipage}%
	}%
	\subfigure[ SSD]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig14b.png}
		\end{minipage}%
	}%
	\subfigure[YOLOv5]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig14c.png}
		\end{minipage}%
	}%
 \\
	\subfigure[YOLOv6]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig14d.png}
		\end{minipage}%
	}%
        \subfigure[ YOLOv7]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig14e.png}
		\end{minipage}%
	}%
	\subfigure[ Ours]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig14f.png}
		\end{minipage}%
	}%	
	\centering
	\caption{The comparative test results}
\end{figure}


\begin{figure}[H]
	\centering
	\subfigure[ Faster R-CNN]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig15a.png}
		\end{minipage}%
	}%
	\subfigure[ SSD]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig15b.png}
		\end{minipage}%
	}%
	\subfigure[YOLOv5]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig15c.png}
		\end{minipage}%
	}%
 \\
	\subfigure[YOLOv6]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig15d.png}
		\end{minipage}%
	}%
        \subfigure[ YOLOv7]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig15e.png}
		\end{minipage}%
	}%
	\subfigure[ Ours]{
		\begin{minipage}[t]{0.32\linewidth}
			\centering
			\includegraphics[scale=0.29]{fig15f.png}
		\end{minipage}%
	}%	
	\centering
	\caption{The comparative test results}
\end{figure}



%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

\end{appendices}

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. please ignore this.             %%
%%===================================================%%


%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.



%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
